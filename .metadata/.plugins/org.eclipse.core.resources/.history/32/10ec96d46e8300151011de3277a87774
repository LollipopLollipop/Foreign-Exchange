import java.util.Arrays;

import org.apache.cassandra.hadoop.ColumnFamilyInputFormat;
import org.apache.cassandra.hadoop.ConfigHelper;
import org.apache.cassandra.thrift.SlicePredicate;
import org.apache.cassandra.utils.ByteBufferUtil;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class HW5 {
	
	private static final Logger logger = LoggerFactory.getLogger(HW5.class);

    static final String KEYSPACE = "dev";
    static final String COLUMN_FAMILY = "input_words";

    //static final String OUTPUT_REDUCER_VAR = "output_reducer";
    //static final String OUTPUT_COLUMN_FAMILY = "output_words";
    private static final String OUTPUT_PATH_PREFIX = "word_count";

    private static final String CONF_COLUMN_NAME = "book";

	public static void main(String[] args) throws Exception{
//		Configuration conf = new Configuration();
//		//conf.setFloat("mapreduce.job.reduce.slowstart.completedmaps", (float) 1.00);
//		Job job = new Job(conf, "RandomForest");
//		job.setJarByClass(MRRandomForest.class);
//		job.setOutputKeyClass(Text.class);
//		job.setOutputValueClass(IntWritable.class);
//		job.setMapperClass(MRRandomForest.TokenizerMapper.class);
//		//job.setCombinerClass(NB_train_hadoop.IntSumCombiner.class);
//		job.setReducerClass(MRRandomForest.IntSumReducer.class);
//		//job.setNumReduceTasks(Integer.parseInt(args[2]));
//		
//		FileInputFormat.addInputPath(job, new Path(args[0]));
//		FileOutputFormat.setOutputPath(job, new Path(args[1]));
//		
//		
//		job.waitForCompletion(true);
		
		
		Configuration conf = new Configuration();
    	String outputReducerType = "filesystem";

        logger.info("output reducer type: " + outputReducerType);

        String columnName = "text";
        conf.set(CONF_COLUMN_NAME, columnName);

        Job job = new Job(conf, "wordcount");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCount.TokenizerMapper.class);
        //job.setCombinerClass(WordCount.ReducerToFilesystem.class);
        job.setReducerClass(WordCount.ReducerToFilesystem.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX));

        job.setInputFormatClass(ColumnFamilyInputFormat.class);


        ConfigHelper.setRpcPort(job.getConfiguration(), "9160");
        ConfigHelper.setInitialAddress(job.getConfiguration(), "localhost");
        ConfigHelper.setPartitioner(job.getConfiguration(), "org.apache.cassandra.dht.RandomPartitioner");
        ConfigHelper.setInputColumnFamily(job.getConfiguration(), KEYSPACE, COLUMN_FAMILY);
//        SlicePredicate predicate = 
//        		new SlicePredicate().setColumn_names(Arrays.asList(ByteBufferUtil.bytes(columnName)));
////            SlicePredicate predicate = 
////            		new SlicePredicate().setColumn_names(Arrays.asList(columnName.getBytes()));
//        
//        ConfigHelper.setInputSlicePredicate(job.getConfiguration(), predicate);

        job.waitForCompletion(true);
	}

}
